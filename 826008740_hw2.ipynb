{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2019\n",
    "\n",
    "\n",
    "# Homework 2:   word2vec + SVM + Evaluation\n",
    "\n",
    "### 100 points [6% of your final grade]\n",
    "\n",
    "### Due: Tuesday, February 26, 2019 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Understand word2vec-like term embeddings,  explore real-world challenges with SVM-based classifiers, understand and implement several evaluation metrics.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw2.ipynb`. For example, my homework submission would be something like `555001234_hw2.ipynb`. Submit this notebook via eCampus (look for the homework 2 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Term embeddings + SVM (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "\n",
    "For this homework, we will still play with Yelp reviews from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge). As in Homework 1, you'll see that each line corresponds to a review on a particular business. Each review has a unique \"ID\" and the text content is in the \"review\" field. Additionally, this time, we also offer you the \"label\". If `label=1`, it means that this review is `Food-relevant`. If `label=0`, it means that this review is `Food-irrelevant`. Similarly, we have already done some basic preprocessing on the reviews, so you can just tokenize each review using whitespace.\n",
    "\n",
    "There are about 40,000 reviews in total, in which about 20,000 reviews are \"Food-irrelevant\". We split the review data into two sets. *review_train.json* is the training set. *review_test.json* is the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': 'not a bad place to brush up on skills or just go to practice they need to organize their classes better with the rotations keep couples in the center and all guests in a circle instructors also need to ensure lessons are taught at the appropriate level do not overwhelm your new guests better than fat cat but nowhere near the level of fred astaire', 'id': 'INxgoY8zIyKAfneid4gCIw', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# Please load the dataset\n",
    "# Your code below\n",
    "import json\n",
    "reviewTrain = []\n",
    "with open('hw2_data/review_train.json','r') as file :\n",
    "    for line in file :\n",
    "        reviewTrain.append(json.loads(line))\n",
    "trainLen = len(reviewTrain)\n",
    "# print((reviewTrain[0]))   type : dict \n",
    "reviewTest = []\n",
    "with open('hw2_data/review_test.json','r') as file :\n",
    "    for line in file :\n",
    "        reviewTest.append(json.loads(line))\n",
    "testLen = len(reviewTest)\n",
    "print(reviewTest[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-trained term embeddings\n",
    "\n",
    "To save your time, you can make use of  pre-trained term embeddings. In this homework, we are using one of the great pre-trained models from [GloVe](https://nlp.stanford.edu/projects/glove/) based on 2 billion tweets. GloVe is quite similar to word2vec. Unzip the *glove.6B.50d.txt.zip* file and run the code below. You will be able to load the term embeddings model, with which each word can be represented with a 50-dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the pre-trained term embeddings\n",
    "import numpy as np\n",
    "with open(\"hw2_data/glove.6B.50d.txt\", \"r\") as lines:\n",
    "    model = {line.split()[0]: np.array(list(map(float, line.split()[1:])))\n",
    "           for line in lines}\n",
    "# print((model['the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have a vector representation for each word. First, we use the simple (arithmetic) **mean** of these vectors of words in a review to represent the review. *Note: Just ignore those words which are not in the corpus of this pre-trained model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please figure out the vector representation for each review in the training data and testing data.\n",
    "# Your code below\n",
    "def getList(target,model) :\n",
    "    res = []\n",
    "    for review in target :\n",
    "        a = np.zeros(50)\n",
    "        reviewLen = len(review['review'])\n",
    "        temp = review['review'].split()\n",
    "        for word in temp :\n",
    "            if (word in model) :\n",
    "                a = a + model[word]\n",
    "        a = a / reviewLen\n",
    "        res.append(a)\n",
    "    return res\n",
    "\n",
    "XTrain = getList(reviewTrain, model)\n",
    "XTest = getList(reviewTest, model)\n",
    "# print(XTrain[0])\n",
    "# print(XTest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.empty([trainLen,50])\n",
    "# i = 0\n",
    "# while i < len(XTrain) :\n",
    "#     X_train[i] = np.ndarray.tolist(XTrain[i])\n",
    "#     i += 1\n",
    "# # print(X_train[0])\n",
    "\n",
    "# X_test = np.empty([testLen,50])\n",
    "# i = 0\n",
    "# while i < len(XTest) :\n",
    "#     X_test[i] = np.ndarray.tolist(XTest[i])\n",
    "#     i += 1\n",
    "# # print(X_test[0])\n",
    "\n",
    "Y_train = []\n",
    "for review in reviewTrain:\n",
    "    Y_train.append(review['label'])\n",
    "\n",
    "Y_test = []\n",
    "for review in reviewTest:\n",
    "    Y_test.append(review['label'])\n",
    "# print(Y_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "With the vector representations you get for each review, please train an SVM model to predict whether a given review is food-relevant or not. **You do not need to implement any classifier from scratch. You may use scikit-learn's built-in capabilities.** You can only train your model with reviews in *review_train.json*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM model training\n",
    "# Your code here\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "clf = svm.SVC(kernel = 'linear')\n",
    "# clf = svm.SVC()\n",
    "clf.fit(XTrain, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to predict whether a given review is food-relevant or not. Please report the overall accuracy, precision and recall of your model on the **testing data**. You should **implement the functions for accuracy, precision, and recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum(pred_clf))\n",
    "\n",
    "# get the n number of 0 in Y_test\n",
    "Y_test0 = 0\n",
    "Y_test1 = 0\n",
    "for num in Y_test :\n",
    "    if num == 0 :\n",
    "        Y_test0 += 1\n",
    "    else :\n",
    "        Y_test1 += 1\n",
    "# print(Y_test0)   5975\n",
    "# print(Y_test1)     5945\n",
    "\n",
    "pre0Correct = 0\n",
    "pre1Correct = 0\n",
    "k = 0 \n",
    "while k < len(Y_test) :\n",
    "    if Y_test[k] == pred_clf[k] and Y_test[k] == 0 :\n",
    "        pre0Correct += 1\n",
    "    elif Y_test[k] == pred_clf[k] and Y_test[k] == 1 :\n",
    "        pre1Correct += 1\n",
    "    k += 1\n",
    "# print(pre0Correct)  5146\n",
    "# print(pre1Correct)  5553 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.8975671140939597 \n",
      "recall is 0.8976587336498094 \n",
      "score is 0.8976129215339772 \n"
     ]
    }
   ],
   "source": [
    "# Precision : \n",
    "precision = (pre0Correct + pre1Correct) / len(Y_test)\n",
    "print('precision is %s '% precision)\n",
    "\n",
    "# recall :\n",
    "recall = (pre0Correct / Y_test0 + pre1Correct / Y_test1) / 2\n",
    "print('recall is %s '% recall)\n",
    "\n",
    "# f score:\n",
    "f = 2 * precision * recall / (precision + recall)\n",
    "print('score is %s '% f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.897567114094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_test, pred_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.86      0.89      5975\n",
      "          1       0.87      0.93      0.90      5945\n",
      "\n",
      "avg / total       0.90      0.90      0.90     11920\n",
      "\n",
      "[[5146  829]\n",
      " [ 392 5553]]\n"
     ]
    }
   ],
   "source": [
    "# this result is used to verify my function's correctness\n",
    "pred_clf = clf.predict(XTest)\n",
    "# print(pred_clf)\n",
    "len(pred_clf)\n",
    "print(classification_report(Y_test, pred_clf))\n",
    "print(confusion_matrix(Y_test, pred_clf))\n",
    "\n",
    "# A B         recall 0 = A / (A + B)\n",
    "# C D         recall 1 = D / (D + C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-based embeddings\n",
    "\n",
    "Instead of taking the mean of term embeddings, you can directly train a **doc2vec** model for paragraph or document embeddings. You can refer to the paper [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053v2.pdf) for more details. And in this homework, you can make use of the implementation in [gensim](https://radimrehurek.com/gensim/models/doc2vec.html).\n",
    "\n",
    "Now, you need to:\n",
    "* Train a doc2vec model based on all reviews you have (training + testing sets).\n",
    "* Use the embeddings from your doc2vec model to represent each review and train a new SVM model.\n",
    "* Report the overall accuracy, precision and recall of your model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiangbohuai/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiangbohuai/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "Model2 Saved\n"
     ]
    }
   ],
   "source": [
    "# Train a doc2vec\n",
    "# Your code here\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "newReviewTrain = []\n",
    "for review in reviewTrain :\n",
    "    newReviewTrain.append(review['review'].split())\n",
    "for review in reviewTest :\n",
    "    newReviewTrain.append(review['review'].split())\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(newReviewTrain)]\n",
    "# print(documents[0:2])\n",
    "max_epochs = 40\n",
    "vec_size = 50\n",
    "alpha = 0.025\n",
    "\n",
    "model2 = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "model2.build_vocab(documents)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model2.train(documents,\n",
    "                total_examples=model2.corpus_count,\n",
    "                epochs=model2.iter)\n",
    "    # decrease the learning rate\n",
    "    model2.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model2.min_alpha = model2.alpha\n",
    "model2.save(\"doc2vec.model\")\n",
    "print(\"Model2 Saved\")    \n",
    "# model = Doc2Vec(documents, vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "# model = Doc2Vec(size=vec_size,alpha=alpha, min_alpha=0.00025,min_count=1,dm =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getList2(target, model2) :\n",
    "    res = []\n",
    "    for review in target :\n",
    "        a = np.zeros(50)\n",
    "        reviewLen = len(review['review'])\n",
    "        temp = review['review'].split()\n",
    "        for word in temp:\n",
    "            a = a + model2[word]\n",
    "        a = a / reviewLen\n",
    "        res.append(a)\n",
    "    return res\n",
    "XTrain2 = getList2(reviewTrain, model2)\n",
    "XTest2 = getList2(reviewTest, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a SVM\n",
    "# Your code here\n",
    "dbe = svm.SVC(kernel = 'linear')\n",
    "dbe.fit(XTrain2, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.88      0.91      5975\n",
      "          1       0.89      0.93      0.91      5945\n",
      "\n",
      "avg / total       0.91      0.91      0.91     11920\n",
      "\n",
      "[[5283  692]\n",
      " [ 413 5532]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90729865771812079"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report the performance\n",
    "# Your code here\n",
    "pred_dbe = dbe.predict(XTest2)\n",
    "print(classification_report(Y_test, pred_dbe))\n",
    "print(confusion_matrix(Y_test, pred_dbe))\n",
    "accuracy = accuracy_score(Y_test, pred_dbe)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? How different are your results for the term-based average approach vs. the doc2vec approach? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*provide a brief (1-2 paragraph) discussion based on these questions.*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the doc2vec approach has better result, althouth not that obvious.\n",
    "the doc2vec approach is 1 percent better.\n",
    "I think it is because the model is better. The reason why it is better is that it includes all the words we need, \n",
    "thus it is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you do better?\n",
    "\n",
    "Finally, see if you can do better than either the word- or doc- based embeddings approach for classification. You may explore new features, new classifiers, etc. Whatever you like. Just provide your code and a justification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## First Try : Neural Network\n",
    "## Result : Neural Network has 0.91124 accuracy, which is better than 0.9073 got above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpc = MLPClassifier(hidden_layer_sizes = (11,11,11), max_iter = 500)\n",
    "mlpc.fit(XTrain, Y_train)\n",
    "pred_mlpc = mlpc.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is 0.911241610738\n"
     ]
    }
   ],
   "source": [
    "# print(classification_report(Y_test, pred_mlpc))\n",
    "# print(confusion_matrix(Y_test, pred_mlpc))\n",
    "accuracy_mlpc = accuracy_score(Y_test, pred_mlpc)\n",
    "print('the accuracy is', accuracy_mlpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Try : Random Forest Classifier : it does not work better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 200)\n",
    "rfc.fit(XTrain, Y_train)\n",
    "pred_rfc = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.86      0.89      5975\n",
      "          1       0.87      0.94      0.90      5945\n",
      "\n",
      "avg / total       0.90      0.90      0.90     11920\n",
      "\n",
      "[[5112  863]\n",
      " [ 380 5565]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89572147651006706"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(Y_test, pred_rfc))\n",
    "print(confusion_matrix(Y_test, pred_rfc))\n",
    "accuracyRFC = accuracy_score(Y_test, pred_rfc)\n",
    "accuracyRFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: NDCG (20 points)\n",
    "\n",
    "You calculated the recall and precision in Part 1 and now you get a chance to implement NDCG. \n",
    "\n",
    "Assume that Amy searches for \"food-relevant\" reviews in the **testing set** on two search engines `A` and `B`. Since the ground-truth labels for the reviews are unknown to A and B, they need to make a prediction for each review and then return a ranked list of results based on their probabilities. The results from A are in *search_result_A.json*, and the results from B are in *search_result_B.json*. Each line contains the id of a review and its corresponding ranking.\n",
    "\n",
    "You can check their labels in *review_test.json* while calculating the NDCG scores. If a review is \"food-relevant\", the relevance score is 1. Otherwise, the relevance score is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG for search_result_A.json\n",
    "# Your code here\n",
    "import math\n",
    "resultA = []\n",
    "with open('hw2_data/search_result_A.json','r') as file :\n",
    "    for line in file :\n",
    "        resultA.append(json.loads(line))\n",
    "resultB = []\n",
    "with open('hw2_data/search_result_B.json','r') as file :\n",
    "    for line in file :\n",
    "        resultB.append(json.loads(line))\n",
    "        \n",
    "def getDCG(search_result) :\n",
    "    DCG = 0\n",
    "    id1 = search_result[0]['id']\n",
    "    for review in reviewTest :\n",
    "        if review['id'] == id1 :\n",
    "            rel = review['label']\n",
    "            search_result[0]['label'] = rel\n",
    "            DCG += review['label']\n",
    "    # print(DCG)\n",
    "    i = 2\n",
    "    while i <= len(search_result) :\n",
    "        rankInfo = search_result[i - 1]\n",
    "        for review in reviewTest :\n",
    "            if review['id'] == rankInfo['id'] :\n",
    "                rel = review['label']\n",
    "                DCG += (rel / math.log(i,2))\n",
    "        rankInfo['label'] = rel\n",
    "        i += 1\n",
    "    return DCG, search_result\n",
    "\n",
    "def getIDCG(search_result) :\n",
    "    search_result = sorted(search_result, key = lambda i : i['label'],reverse = True)\n",
    "    IDCG = 1\n",
    "    i = 2\n",
    "    while i <= len(search_result) :\n",
    "        rankInfo = search_result[i - 1]\n",
    "        IDCG += rankInfo['label'] / math.log(i, 2)\n",
    "        i += 1\n",
    "    return IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NDCG of search engine A is 0.9689322925726681\n"
     ]
    }
   ],
   "source": [
    "# print(getDCG(resultA).search_result)\n",
    "# print(getIDCG(resultA))\n",
    "DCG_A = getDCG(resultA)[0]\n",
    "# print(DCG1)\n",
    "resultA = getDCG(resultA)[1]\n",
    "IDCG_A = getIDCG(resultA)\n",
    "print('The NDCG of search engine A is %s' % (DCG_A / IDCG_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NDCG of search engine B is 0.9971789169738982\n"
     ]
    }
   ],
   "source": [
    "# NDCG for search_result_B.json\n",
    "# Your code here\n",
    "DCG_B = getDCG(resultB)[0]\n",
    "resultB = getDCG(resultB)[1]\n",
    "IDCG_B = getIDCG(resultB)\n",
    "print('The NDCG of search engine B is %s' % (DCG_B / IDCG_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=0Lt9w-BxKFQ&t=1988s         neural network and Random Forest Classifier\n",
    "# https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5     gensim tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
